{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRDF fitting and rendering (Provided_Capture/ reference images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will fit BRDFs to the processed images. The BRDFs will be rendered to check if they are faithful to the captured images and also quantiatively evaluated\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '1'\n",
    "import torch\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ipywidgets import *\n",
    "from torch import nn\n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.optimize\n",
    "import imageio.v2 as imageio"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Height & Width of the Grid\n",
    "H, W = 256, 256\n",
    "ball_radius = 340  # in mm  (Replace this with the size of your sphere)\n",
    "light_dist = 1000 / ball_radius  # Distance of the light from the object.\n",
    "camera_dist = 1850 / ball_radius  # Distance of the Camera from the object."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Normalized Meshgrid with coordinates from -1 to 1. \n",
    "y, x = np.meshgrid(np.linspace(-1, 1, H), np.linspace(-1, 1, W))\n",
    "\n",
    "# Mask for the pixels on the sphere considering it is a Unit Sphere (with a center at 0, 0 and a radius of 1)\n",
    "in_frame = x**2 + y**2 <= 1\n",
    "\n",
    "# Compute the z corresponding to each pixel. \n",
    "# Assume z is 0 in the center of the sphere and increases towards the camera\n",
    "# z = sqrt(1 - x^2 - y^2) for points inside the sphere, else z = 0.\n",
    "z = np.zeros_like(x)\n",
    "z[in_frame] = np.sqrt(1 - x[in_frame]**2 - y[in_frame]**2)\n",
    "\n",
    "# Compute the normals corresponding to each pixel for the unit sphere. \n",
    "# The normals are simply the normalized coordinates of the sphere surface.\n",
    "normals = np.zeros((H, W, 3))\n",
    "normals[..., 0] = x  # X-component\n",
    "normals[..., 1] = y  # Y-component\n",
    "normals[..., 2] = z  # Z-component\n",
    "normals[in_frame] /= np.linalg.norm(normals[in_frame], axis=-1, keepdims=True)  # Normalize the vectors only for in-frame points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(z)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(normals / 2 + 0.5);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a simple BRDF model. Each BRDF instance should be callable, with two arguments: w_i, and w_o, which are the normalized vectors (omegas) describing the incident and and viewing directions respectively. The idea is to calculate the BRDF for all the points of the sphere at once (vectorized implementation) instead of finding the BRDF value for each point over a loop."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LambertianBRDF:\n",
    "    # Simple example for a Lambertian BRDF for virtual object (Green for instance)\n",
    "    def __init__(self, diffuse_color=(0.0, 1.0, 0.0)):\n",
    "        self.diffuse_color = np.array(diffuse_color)\n",
    "\n",
    "    def __call__(self, w_i: np.ndarray, w_o: np.ndarray) -> np.ndarray:\n",
    "        return np.broadcast_to(self.diffuse_color, w_i.shape)  # Broadcast is used to attend to the B x 3 expected shape of the directions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the PhongBRDF in a similar format to the provided LambertianBRDF\n",
    "class PhongBRDF:\n",
    "    def __init__(self, diffuse_color=(0.0, 1.0, 0.0), specular_color=(1.0, 1.0, 1.0), alpha=20.0):\n",
    "        self.diffuse_color = np.array(diffuse_color)\n",
    "        self.specular_color = np.array(specular_color)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, w_i: np.ndarray, w_o: np.ndarray) -> np.ndarray:\n",
    "        # Calculate the halfway vector\n",
    "        h = (w_i + w_o) / np.linalg.norm(w_i + w_o, axis=-1, keepdims=True)\n",
    "        # Compute the diffuse term\n",
    "        diffuse = np.broadcast_to(self.diffuse_color, w_i.shape)\n",
    "        # Compute the specular term using the Phong reflection model\n",
    "        specular = self.specular_color * (np.maximum(np.sum(h * w_o, axis=-1, keepdims=True), 0.0) ** self.alpha)\n",
    "        # Return the combination of diffuse and specular components\n",
    "        return diffuse + specular"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Utility function to normalize a vector\n",
    "def normalize(v):\n",
    "    \"\"\" Utility function that normalizes a vector. Expects shape ... x 3 \"\"\"\n",
    "    norm = np.linalg.norm(v, axis=-1, keepdims=True)\n",
    "    return np.where(norm == 0, v, v / norm)  # Avoid division by zero by returning the original vector if norm is zero"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#############################################################################################2nd\n",
    "# Utility function to compute rotations\n",
    "def get_rotations(a, b=(0.0, 0.0, 1.0)):\n",
    "    \"\"\" Utility function that computes the rotation to bring a vector _a_ to direction _b_. \n",
    "        Hint: Keep in mind we are processing the points in parallel manner. That is, shape of vector a is B x 3    \n",
    "    \"\"\"\n",
    "    a = normalize(a)\n",
    "    b = normalize(np.array(b))\n",
    "    v = np.cross(a, b)\n",
    "    v_norm = np.linalg.norm(v, axis=-1, keepdims=True)\n",
    "    skew = np.zeros((a.shape[0], 3, 3))\n",
    "    skew[:, 0, 1] = -v[..., 2]\n",
    "    skew[:, 0, 2] = v[..., 1]\n",
    "    skew[:, 1, 0] = v[..., 2]\n",
    "    skew[:, 1, 2] = -v[..., 0]\n",
    "    skew[:, 2, 0] = -v[..., 1]\n",
    "    skew[:, 2, 1] = v[..., 0]\n",
    "\n",
    "        # Identity matrix for rotation matrix calculation\n",
    "    i = np.eye(3).reshape(1, 3, 3)  # Shape: (1, 3, 3)\n",
    "    i = np.broadcast_to(i, skew.shape)  # Shape: (B, 3, 3)\n",
    "    # Cosine of the angle between `a` and `b`\n",
    "    cos = np.einsum('ij,j->i', a, b)  # Dot product between `a` and `b`, shape (B,)\n",
    "\n",
    "\n",
    "    # Rodrigues' rotation formula\n",
    "    rotations = i + skew + np.matmul(skew, skew) * (1 / (1 + cos[:, np.newaxis, np.newaxis]))\n",
    "    return rotations  # B x 3 x 3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def render_sphere_image(brdf, light_angle: float, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Renders the sphere given a light direction and optionally saves it as an image.\n",
    "    Args:\n",
    "        brdf: The BRDF model to render.\n",
    "        light_angle: Float giving the light direction in degrees.\n",
    "        save_path: Optional. Path to save the image. If None, the image is not saved.\n",
    "    \"\"\"\n",
    "    # Convert light angle to radians and compute light and camera positions\n",
    "    light_angle = np.deg2rad(light_angle)\n",
    "    light_pos = light_dist * np.array([0, np.sin(light_angle), np.cos(light_angle)])\n",
    "    camera_pos = camera_dist * np.array([0, 0, 1])\n",
    "\n",
    "    # Generate grid of normalized coordinates for the unit sphere\n",
    "    y, x = np.meshgrid(np.linspace(-1, 1, H), np.linspace(-1, 1, W))\n",
    "    in_frame = x**2 + y**2 <= 1  # Mask for points within the unit sphere\n",
    "\n",
    "    # Flatten x, y, and compute z values for the unit sphere\n",
    "    x_flat = x.flatten()\n",
    "    y_flat = y.flatten()\n",
    "    z_flat = np.sqrt(1 - x_flat**2 - y_flat**2)\n",
    "    z_flat[~in_frame.flatten()] = 0  # Set outside points to zero\n",
    "    \n",
    "    points = np.stack([x_flat, y_flat, z_flat], axis=-1)  # Shape: (B, 3)\n",
    "    points = points[in_frame.flatten()]  # Only use points within the sphere\n",
    "\n",
    "    # Normals at each point on the sphere's surface (same as points for a unit sphere)\n",
    "    normals = points  # Shape: (B, 3)\n",
    "\n",
    "    # Direction from each point to the light source\n",
    "    dir_to_light = normalize(light_pos - points)  # Shape: (B, 3)\n",
    "\n",
    "    # Direction from each point to the camera\n",
    "    dir_to_camera = normalize(camera_pos - points)  # Shape: (B, 3)\n",
    "\n",
    "    # Rotate directions into a canonical space using rotations that align each normal to the z-axis\n",
    "    rotations = get_rotations(normals)  # Shape: (B, 3, 3)\n",
    "\n",
    "    # Rotate `dir_to_light` and `dir_to_camera` to align with the canonical space\n",
    "    w_i = np.einsum('bij,bj->bi', rotations, dir_to_light)  # Shape: (B, 3)\n",
    "    w_o = np.einsum('bij,bj->bi', rotations, dir_to_camera)  # Shape: (B, 3)\n",
    "\n",
    "    # Cosine term between the light direction and the normal at each point\n",
    "    cos = np.einsum('bi,bi->b', dir_to_light, normals)[:, np.newaxis]  # Shape: (B, 1)\n",
    "    cos = np.clip(cos, 0, 1)  # Ensure the cosine is non-negative\n",
    "\n",
    "    # Evaluate the BRDF\n",
    "    light_out = brdf(w_i, w_o) * cos  # Apply BRDF with cosine term for light attenuation\n",
    "\n",
    "    # Reshape to the image format\n",
    "    light_out_img = np.zeros((H * W, 3))  # Initialize with zero for all pixels\n",
    "    light_out_img[in_frame.flatten()] = light_out  # Only assign to sphere pixels\n",
    "\n",
    "    # Set pixels outside the sphere to black (0)\n",
    "    light_out_img = light_out_img.reshape(H, W, 3)\n",
    "    light_out_img[~in_frame] = 0  # Set pixels outside the sphere to black\n",
    "    \n",
    "    # Normalize image to [0, 255] for saving (optional)\n",
    "    light_out_img_normalized = np.clip(light_out_img, 0, 1) * 255\n",
    "    light_out_img_normalized = light_out_img_normalized.astype(np.uint8)\n",
    "    \n",
    "    # Save the image if a path is provided\n",
    "    if save_path:\n",
    "        imageio.imwrite(save_path, light_out_img_normalized)\n",
    "        print(f\"Image saved to {save_path}\")\n",
    "\n",
    "    return light_out_img\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def linear_to_srgb(img):\n",
    "\tlimit = 0.0031308\n",
    "\treturn np.where(img > limit, 1.055 * (img ** (1.0 / 2.4)) - 0.055, 12.92 * img)\n",
    "\n",
    "def plot_hdr(image, title=None):\n",
    "    srgb = linear_to_srgb(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.clip(srgb, 0, 1))\n",
    "    \n",
    "def create_render(brdf):\n",
    "    def render(light_angle):\n",
    "        image = render_sphere_image(brdf, light_angle)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.title(\"Rendered BRDF\")\n",
    "        plt.axis('off')\n",
    "        plot_hdr(image)\n",
    "    return render"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "brdf = PhongBRDF()\n",
    "interact(create_render(brdf), brdf=brdf, light_angle=(-120, 120, 1.0));\n",
    "# def render(brdf, light_angle):\n",
    "#     image = render_sphere_image(brdf, light_angle)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.title(\"Rendered BRDF\")\n",
    "#     plt.axis('off')\n",
    "#     plot_hdr(image)\n",
    "# interact(lambda light_angle: render(brdf, light_angle), light_angle=(-120, 120, 1.0))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "result = render_sphere_image(LambertianBRDF(), 20.0)\n",
    "plot_hdr(result, \"Lambertian BRDF\")\n",
    "plt.subplot(1, 2, 2)\n",
    "result = render_sphere_image(PhongBRDF(), 20.0)\n",
    "plot_hdr(result, \"Phong BRDF\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We defined and render static, manually adjusted BRDFs. Now will try to infere BRDFs from data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Loading paths\n",
    "\n",
    "\n",
    "\n",
    "data_dir= Path(\"ProvidedCapture_01\")\n",
    "angles = [5, 20, 35, 50, 65, 80, 95, 110, 125, 140, 155, 170] \n",
    "paths = [data_dir / \"crops\" / f\"angle_{angle}.exr\" for angle in angles]\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "assert len(paths) > 3 and all(p.exists() for p in paths)\n",
    "\n",
    "# hdr_crops = "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "hdr_crops = []\n",
    "\n",
    "for path in paths:\n",
    "    # Load the HDR image using OpenCV's imread function, which supports .exr files\n",
    "    hdr_image = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    if hdr_image is None:\n",
    "        raise ValueError(f\"Could not load image at {path}\")\n",
    "    \n",
    "    # Resize to a standard size if needed (e.g., 256x256)\n",
    "    hdr_image_resized = cv2.resize(hdr_image, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Add to list\n",
    "    hdr_crops.append(hdr_image_resized)\n",
    "\n",
    "# Convert list to numpy array for batch processing if desired\n",
    "hdr_crops = np.stack(hdr_crops)  # Shape: (len(paths), 256, 256, 3) assuming RGB HDR images"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_hdr(hdr_crops[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def collect_brdf_data(input_image, light_angle=0.0, cos_eps=0.01):\n",
    "    \"\"\"\n",
    "    Aggregates BRDF information from the input image, observing a certain light angle.\n",
    "    Args:\n",
    "        input_image: HDR image of the rendered sphere.\n",
    "        light_angle: Float giving the light direction in degrees.\n",
    "        cos_eps: Float threshold to ignore unreliable information (cos < cos_eps).\n",
    "    Returns:\n",
    "        Tuple of 3 np.arrays of shape (B, 3): w_i, w_o, and pixel_colors for valid pixels.\n",
    "    \"\"\"\n",
    "    # Convert light angle to radians and determine the light position\n",
    "    light_angle = np.deg2rad(light_angle)\n",
    "    light_pos = light_dist * np.array([0, np.sin(light_angle), np.cos(light_angle)])\n",
    "    camera_pos = camera_dist * np.array([0, 0, 1])\n",
    "\n",
    "    # Flatten x, y, z coordinates to B x 3 format\n",
    "    x_flat = x.flatten()\n",
    "    y_flat = y.flatten()\n",
    "    z_flat = z.flatten()\n",
    "    points = np.stack([x_flat, y_flat, z_flat], axis=-1)  # Shape: (B, 3)\n",
    "    points = points[in_frame.flatten()]  # Only points on the sphere surface\n",
    "\n",
    "    # Surface normals are the same as points for a unit sphere centered at the origin\n",
    "    normals = points  # Shape: (B_sphere, 3)\n",
    "\n",
    "    # Direction to the light source\n",
    "    dir_to_light = normalize(light_pos - points)  # Shape: (B_sphere, 3)\n",
    "\n",
    "    # Direction to the camera\n",
    "    dir_to_camera = normalize(camera_pos - points)  # Shape: (B_sphere, 3)\n",
    "\n",
    "    # Calculate cosine of the angle between the normal and the light direction\n",
    "    cos_theta = np.einsum('ij,ij->i', dir_to_light, normals)  # Shape: (B_sphere,)\n",
    "    valid_mask = cos_theta >= cos_eps  # Only include pixels where cos_theta >= cos_eps\n",
    "\n",
    "    # Apply the mask to filter valid points and directions\n",
    "    w_i = dir_to_light[valid_mask]  # Incident directions for valid points\n",
    "    w_o = dir_to_camera[valid_mask]  # Outgoing directions for valid points\n",
    "\n",
    "    # Extract pixel colors from the input image\n",
    "    input_image_flat = input_image.reshape(-1, 3)  # Flatten image to match point format\n",
    "    pixel_colors = input_image_flat[in_frame.flatten()][valid_mask]  # Filter colors for valid points\n",
    "\n",
    "    return w_i, w_o, pixel_colors\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize lists to store results from each angle\n",
    "w_i_list = []\n",
    "w_o_list = []\n",
    "values_list = []\n",
    "\n",
    "# Iterate over each angle and corresponding HDR crop image\n",
    "for angle, hdr_crop in zip(angles, hdr_crops):\n",
    "    # Collect BRDF data for the current image and angle\n",
    "    w_i_angle, w_o_angle, pixel_colors = collect_brdf_data(hdr_crop, light_angle=angle)\n",
    "\n",
    "    # Append results to the lists\n",
    "    w_i_list.append(w_i_angle)\n",
    "    w_o_list.append(w_o_angle)\n",
    "    values_list.append(pixel_colors)\n",
    "\n",
    "# Concatenate results from all angles into single arrays\n",
    "w_i = np.concatenate(w_i_list, axis=0)  # Shape: (total_valid_points, 3)\n",
    "w_o = np.concatenate(w_o_list, axis=0)  # Shape: (total_valid_points, 3)\n",
    "values = np.concatenate(values_list, axis=0)  # Shape: (total_valid_points, 3)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "assert w_i.shape == w_o.shape == values.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LambertianFitBRDF():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = LambertianBRDF()\n",
    "        self.params = np.array(self.model.diffuse_color)\n",
    "        self.result = None\n",
    "    \n",
    "    def run(self, params, w_i, w_o):\n",
    "        self.model.diffuse_color = params[:3]\n",
    "        return self.model(w_i, w_o)\n",
    "\n",
    "    def objective(self, params, w_i, w_o, values):\n",
    "        prediction = self.run(params, w_i, w_o)\n",
    "        return np.mean((prediction - values) ** 2)\n",
    "\n",
    "    def fit(self, w_i, w_o, values):\n",
    "        self.result = scipy.optimize.minimize(self.objective, x0=self.params, args=(w_i, w_o, values))\n",
    "        self.params = self.result.x\n",
    "        return self\n",
    "\n",
    "    def __call__(self, w_i: np.ndarray, w_o: np.ndarray) -> np.ndarray:\n",
    "        return self.run(self.params, w_i, w_o)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fitted_brdf = LambertianFitBRDF().fit(w_i, w_o, values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "interact(create_render(fitted_brdf), brdf=fitted_brdf, light_angle=(-120, 120, 1.0));"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "plt.figure(figsize=(6, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_hdr(hdr_crops[idx], \"Real image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_hdr(render_sphere_image(fitted_brdf, angles[idx]), \"Fitted Lambertian\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the LambertianFitBRDF, you are required to provide the code for the methods of the PhongFitBRDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "class PhongFitBRDF:\n",
    "    def __init__(self):\n",
    "        # Initialize BRDF parameters: diffuse color (R, G, B), specular color (R, G, B), and shininess (alpha)\n",
    "        self.diffuse_color = np.array([0.5, 0.5, 0.5])  # Initial diffuse color\n",
    "        self.specular_color = np.array([0.5, 0.5, 0.5])  # Initial specular color\n",
    "        self.alpha = 50.0  # Initial shininess coefficient\n",
    "\n",
    "    def run(self, params, w_i, w_o):\n",
    "        \"\"\"\n",
    "        Calculate the Phong BRDF with the current parameters.\n",
    "        Args:\n",
    "            params: Array containing diffuse color (3), specular color (3), and alpha (1).\n",
    "            w_i: Array of incident directions.\n",
    "            w_o: Array of outgoing directions.\n",
    "        Returns:\n",
    "            Calculated BRDF values based on input directions and params.\n",
    "        \"\"\"\n",
    "        # Unpack params\n",
    "        diffuse_color = params[:3]\n",
    "        specular_color = params[3:6]\n",
    "        alpha = params[6]\n",
    "\n",
    "        # Calculate halfway vector\n",
    "        h = normalize((w_i + w_o) / 2)\n",
    "\n",
    "        # Diffuse component (Lambertian reflection)\n",
    "        diffuse = np.broadcast_to(diffuse_color, w_i.shape)\n",
    "\n",
    "        # Specular component (Phong reflection)\n",
    "        # Use a normal vector [0, 0, 1] broadcasted to match the shape of `h`\n",
    "        normal = np.array([0, 0, 1]).reshape(1, 3)\n",
    "        specular_term = np.maximum(np.einsum('ij,ij->i', h, normal), 0.0) ** alpha\n",
    "        specular = specular_color * specular_term[:, np.newaxis]\n",
    "\n",
    "        # Total BRDF\n",
    "        return diffuse + specular\n",
    "\n",
    "    def objective(self, params, w_i, w_o, values):\n",
    "        \"\"\"\n",
    "        Objective function to minimize. Computes the mean squared error between\n",
    "        the predicted BRDF values and actual observed values.\n",
    "        Args:\n",
    "            params: Array of current BRDF parameters.\n",
    "            w_i: Array of incident directions.\n",
    "            w_o: Array of outgoing directions.\n",
    "            values: Observed BRDF values.\n",
    "        Returns:\n",
    "            Mean squared error between predicted and observed BRDF values.\n",
    "        \"\"\"\n",
    "        predictions = self.run(params, w_i, w_o)\n",
    "        error = np.mean((predictions - values) ** 2)\n",
    "        return error\n",
    "\n",
    "    def fit(self, w_i, w_o, values):\n",
    "        \"\"\"\n",
    "        Fit the Phong BRDF model to observed BRDF values.\n",
    "        Args:\n",
    "            w_i: Array of incident directions.\n",
    "            w_o: Array of outgoing directions.\n",
    "            values: Observed BRDF values.\n",
    "        \"\"\"\n",
    "        # Initial parameters: [diffuse_color, specular_color, alpha]\n",
    "        initial_params = np.concatenate([self.diffuse_color, self.specular_color, [self.alpha]])\n",
    "\n",
    "        # Bounds: diffuse and specular colors between [0, 1], alpha between [1, 100]\n",
    "        bounds = [(0, 1)] * 6 + [(1, 200)]\n",
    "\n",
    "        # Minimize the objective function\n",
    "        result = minimize(self.objective, initial_params, args=(w_i, w_o, values), bounds=bounds)\n",
    "\n",
    "        # Update parameters if optimization is successful\n",
    "        if result.success:\n",
    "            self.diffuse_color = result.x[:3]\n",
    "            self.specular_color = result.x[3:6]\n",
    "            self.alpha = result.x[6]\n",
    "        else:\n",
    "            raise RuntimeError(\"Optimization failed: \" + result.message)\n",
    "\n",
    "    def __call__(self, w_i: np.ndarray, w_o: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the BRDF for given directions with the optimized parameters.\n",
    "        Args:\n",
    "            w_i: Incident directions.\n",
    "            w_o: Outgoing directions.\n",
    "        Returns:\n",
    "            Predicted BRDF values based on the optimized parameters.\n",
    "        \"\"\"\n",
    "        # Concatenate current parameters\n",
    "        params = np.concatenate([self.diffuse_color, self.specular_color, [self.alpha]])\n",
    "        return self.run(params, w_i, w_o)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#fitted_brdf = PhongFitBRDF().fit(w_i, w_o, values)\n",
    "fitted_brdf = PhongFitBRDF()\n",
    "fitted_brdf.fit(w_i, w_o, values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "interact(create_render(fitted_brdf), brdf=fitted_brdf, light_angle=(-120, 120, 1.0));"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "plt.figure(figsize=(6, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_hdr(hdr_crops[idx], \"Real image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_hdr(render_sphere_image(fitted_brdf, angles[idx]), \"Fitted Phong\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Phong model fits the data significantly better, it still has several limitations.\n",
    "In this section we will model the BRDF directly using a neural network following the approach proposed by Sztrajman et al. in https://arxiv.org/pdf/2102.05963.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing saving"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Render and save the image with a light angle of 45 degrees\n",
    "rendered_image = render_sphere_image(brdf=fitted_brdf, light_angle=angles[idx], save_path=\"rendered_sphere.png\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def default_transform(w_i, w_o):\n",
    "    return np.concatenate([w_i, w_o], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def to_rusinkiewicz(w_i, w_o):\n",
    "    \"\"\"\n",
    "    Converts incident and outgoing directions to the Rusinkiewicz parameterization.\n",
    "    Args:\n",
    "        w_i: Incident direction vector (shape: B x 3).\n",
    "        w_o: Outgoing direction vector (shape: B x 3).\n",
    "    Returns:\n",
    "        Tuple of np.ndarrays (theta_h, phi_h, theta_d, phi_d) each of shape (B,).\n",
    "    \"\"\"\n",
    "    w_i = normalize(w_i)\n",
    "    w_o = normalize(w_o)\n",
    "    h = normalize((w_i + w_o) / 2)\n",
    "\n",
    "    theta_h = np.arccos(np.clip(h[:, 2], -1.0, 1.0))  # Angle between h and z-axis\n",
    "    phi_h = np.arctan2(h[:, 1], h[:, 0])              # Azimuthal angle of h\n",
    "    theta_d = np.arccos(np.clip(np.einsum('ij,ij->i', w_i, h), -1.0, 1.0))\n",
    "    phi_d = np.arctan2(\n",
    "        np.einsum('ij,ij->i', np.cross(normalize(w_i - h * np.einsum('ij,ij->i', w_i, h)[:, None]), \n",
    "                                       normalize(w_o - h * np.einsum('ij,ij->i', w_o, h)[:, None])), h),\n",
    "        np.einsum('ij,ij->i', normalize(w_i - h * np.einsum('ij,ij->i', w_i, h)[:, None]), \n",
    "                          normalize(w_o - h * np.einsum('ij,ij->i', w_o, h)[:, None]))\n",
    "    )\n",
    "\n",
    "    # Additional features to get a 6-dimensional feature vector\n",
    "    cos_theta_h = np.cos(theta_h)\n",
    "    cos_theta_d = np.cos(theta_d)\n",
    "\n",
    "    return np.column_stack((theta_h, phi_h, theta_d, phi_d, cos_theta_h, cos_theta_d))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class IndicesDataset(Dataset):\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.indices[idx]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class NeuralBRDF(nn.Module):\n",
    "    def __init__(self, device='cpu', input_size=6, transform=None, \n",
    "                 learning_rate=5e-4, batch_size=512):\n",
    "        super(NeuralBRDF, self).__init__()\n",
    "        self.device = device\n",
    "        self.transform = transform if transform else default_transform\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Model architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3)  # Output layer with 3 channels (RGB)\n",
    "        ).to(device)\n",
    "\n",
    "    def mean_absolute_logarithmic_error(self, y_true, y_pred):\n",
    "        # Custom log-based loss function\n",
    "        return torch.mean(torch.abs(torch.log(1 + y_true) - torch.log(1 + y_pred)))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Run the data through the model + the final exponential activation\n",
    "        return torch.exp(self.model(data))\n",
    "\n",
    "    def _train(self, data, values, weights, epochs):\n",
    "        # Expand weights to match the shape of values\n",
    "        weights = weights[:, None].expand_as(values)\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        indices = IndicesDataset(torch.arange(0, len(data), device=data.device))\n",
    "        data_loader = DataLoader(indices, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            for chunk in data_loader:\n",
    "                # Get mini-batch data\n",
    "                batch_data = data[chunk]\n",
    "                batch_values = values[chunk]\n",
    "                batch_weights = weights[chunk]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(batch_data)\n",
    "                \n",
    "                # Compute log-based loss with weights\n",
    "                loss = (batch_weights * self.mean_absolute_logarithmic_error(batch_values, predictions)).mean()\n",
    "\n",
    "                # Backpropagation and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def fit(self, w_i, w_o, values, epochs=40):\n",
    "        # Transform input directions using the provided transform\n",
    "        data = self.transform(w_i, w_o)\n",
    "\n",
    "        # Ensure data is a single numpy array before converting to a tensor\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "\n",
    "        # Calculate weights as l · n (dot product of w_i and normal vector)\n",
    "        l_dot_n = torch.tensor(np.clip(w_i[..., 2], 1e-5, 1), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Convert data and values to tensors\n",
    "        data = torch.tensor(data, dtype=torch.float32, device=self.device)\n",
    "        values = torch.tensor(values, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Train the model\n",
    "        self._train(data, values, l_dot_n, epochs)\n",
    "        return self\n",
    "\n",
    "    def inference(self, data):\n",
    "        # Run data through the model in evaluation mode\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted = self.forward(data)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "    def __call__(self, w_i: np.ndarray, w_o: np.ndarray) -> np.ndarray:\n",
    "        # Apply the transform to prepare data for inference\n",
    "        data = self.transform(w_i, w_o)\n",
    "        \n",
    "        # Ensure data is a single numpy array before converting to a tensor\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "\n",
    "        # Convert data to a tensor and handle NaN or infinite values\n",
    "        data = torch.tensor(data, dtype=torch.float32, device=self.device)\n",
    "        data[~torch.isfinite(data)] = 0\n",
    "        \n",
    "        # Perform inference\n",
    "        return self.inference(data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fitted_brdf = NeuralBRDF(input_size=6, transform=to_rusinkiewicz).fit(w_i, w_o, values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "interact(create_render(fitted_brdf), brdf=fitted_brdf, light_angle=(-120, 120, 1.0));"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "plt.figure(figsize=(12, 24))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_hdr(hdr_crops[idx], \"Real image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_hdr(render_sphere_image(fitted_brdf, angles[idx]), \"Neural BRDF\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Render and save the image with a light angle of [parametrized] degrees\n",
    "rendered_image = render_sphere_image(brdf=fitted_brdf, light_angle=angles[idx], save_path=\"neural_sphere.png\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Rusinkiewicz Parameterization with Isotropic Constraints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_rusinkiewicz_isotropic(w_i, w_o):\n",
    "    \"\"\"\n",
    "    Converts incident and outgoing directions to a modified Rusinkiewicz parameterization\n",
    "    that enforces isotropy and Helmholtz reciprocity.\n",
    "    \n",
    "    Args:\n",
    "        w_i: Incident direction vector (shape: B x 3).\n",
    "        w_o: Outgoing direction vector (shape: B x 3).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A 6-dimensional feature vector (theta_h, phi_h, theta_d, phi_d, cos_theta_h, cos_theta_d)\n",
    "                    enforcing isotropy and reciprocity.\n",
    "    \"\"\"\n",
    "    # Normalization of input vectors\n",
    "    w_i = normalize(w_i)\n",
    "    w_o = normalize(w_o)\n",
    "    \n",
    "    # Halfway vector for isotropy (same regardless of w_i and w_o order)\n",
    "    h = normalize((w_i + w_o) / 2)\n",
    "    \n",
    "    # Calculate angles for the Rusinkiewicz parameterization\n",
    "    theta_h = np.arccos(np.clip(h[:, 2], -1.0, 1.0))  # Angle between h and z-axis (isotropic)\n",
    "    phi_h = np.arctan2(h[:, 1], h[:, 0])              # Azimuthal angle of h\n",
    "    \n",
    "    # Dot products to calculate theta_d with reciprocity\n",
    "    cos_theta_d = np.einsum('ij,ij->i', w_i, h)\n",
    "    theta_d = np.arccos(np.clip(cos_theta_d, -1.0, 1.0))\n",
    "\n",
    "    # For isotropy, adjust phi_d calculation to be symmetric with respect to w_i and w_o\n",
    "    w_i_proj = normalize(w_i - h * cos_theta_d[:, None])\n",
    "    w_o_proj = normalize(w_o - h * np.einsum('ij,ij->i', w_o, h)[:, None])\n",
    "\n",
    "    phi_d = np.arctan2(\n",
    "        np.einsum('ij,ij->i', np.cross(w_i_proj, w_o_proj), h),\n",
    "        np.einsum('ij,ij->i', w_i_proj, w_o_proj)\n",
    "    )\n",
    "\n",
    "    # Additional features to form a 6-dimensional feature vector\n",
    "    cos_theta_h = np.cos(theta_h)\n",
    "\n",
    "    return np.column_stack((theta_h, phi_h, theta_d, phi_d, cos_theta_h, cos_theta_d))\n",
    "\n",
    "def normalize(vectors):\n",
    "    \"\"\"Normalizes a batch of vectors to unit length. Handles both 1D and 2D inputs.\"\"\"\n",
    "    vectors = np.atleast_2d(vectors)  # Ensure vectors is at least 2D\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / np.clip(norms, 1e-8, None)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fitted_brdf = NeuralBRDF(input_size=6, transform=to_rusinkiewicz_isotropic).fit(w_i, w_o, values)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "interact(create_render(fitted_brdf), brdf=fitted_brdf, light_angle=(-120, 120, 1.0));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you should evaluate the rendered images, comparing them to the camera captures.\n",
    "Visualize and compute metrics for the Phong and Neural fitted BRDFs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from skimage.metrics import mean_squared_error, structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(original_image, generated_image):\n",
    "    \"\"\"\n",
    "    Compute MSE and SSIM metrics between the original and generated images without normalization.\n",
    "\n",
    "    Args:\n",
    "        original_image (np.ndarray): The ground truth image.\n",
    "        generated_image (np.ndarray): The image generated by the BRDF model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing MSE and SSIM values.\n",
    "    \"\"\"\n",
    "    # Determine data range based on image type (e.g., 255 for 8-bit images)\n",
    "    data_range = 255 if original_image.max() > 1 else 1.0\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse_value = mean_squared_error(original_image, generated_image)\n",
    "    ssim_value = ssim(original_image, generated_image, win_size=3, channel_axis=-1, data_range=data_range)\n",
    "\n",
    "    return {\"MSE\": mse_value, \"SSIM\": ssim_value}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Normalize images to range [0, 1]\n",
    "def normalize_image(image):\n",
    "    return np.clip(image / 255.0, 0, 1) if image.max() > 1 else image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phong vs real image (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img1 = normalize_image(hdr_crops[idx])\n",
    "\n",
    "# Load the rendered image and normalize\n",
    "rendered_img = imageio.imread('rendered_sphere.png')\n",
    "img2 = normalize_image(rendered_img)\n",
    "\n",
    "# Ensure shapes match before computing metrics\n",
    "if img1.shape == img2.shape:\n",
    "    phong_metrics = compute_metrics(img1, img2)\n",
    "    print(\"Phong BRDF Metrics:\", phong_metrics)\n",
    "else:\n",
    "    print(\"Error: Image shapes do not match.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img1 = hdr_crops[idx]\n",
    "\n",
    "# Load the rendered image and normalize\n",
    "rendered_img = imageio.imread('rendered_sphere.png')\n",
    "# img2 = normalize_image(rendered_img)\n",
    "\n",
    "# Ensure shapes match before computing metrics\n",
    "if img1.shape == img2.shape:\n",
    "    phong_metrics = compute_metrics(img1, rendered_img)\n",
    "    print(\"Phong BRDF Metrics:\", phong_metrics)\n",
    "else:\n",
    "    print(\"Error: Image shapes do not match.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important to note (wrt SSIM)\n",
    "\n",
    "The Structural Similarity Index (SSIM) is designed to measure perceived visual similarity between two images, focusing on aspects like luminance, contrast, and structure. Because SSIM is a relative measure, it can yield the same result whether images are normalized or not, provided that the data_range is appropriately specified for the image range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural vs real image (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img1 = normalize_image(hdr_crops[idx])\n",
    "\n",
    "# Load the rendered image and normalize\n",
    "rendered_img = imageio.imread('neural_sphere.png')\n",
    "img2 = normalize_image(rendered_img)\n",
    "\n",
    "# Ensure shapes match before computing metrics\n",
    "if img1.shape == img2.shape:\n",
    "    phong_metrics = compute_metrics(img1, img2)\n",
    "    print(\"Neural BRDF Metrics:\", phong_metrics)\n",
    "else:\n",
    "    print(\"Error: Image shapes do not match.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img1 = hdr_crops[idx]\n",
    "\n",
    "# Load the rendered image and normalize\n",
    "rendered_img = imageio.imread('neural_sphere.png')\n",
    "# img2 = normalize_image(rendered_img)\n",
    "\n",
    "# Ensure shapes match before computing metrics\n",
    "if img1.shape == img2.shape:\n",
    "    phong_metrics = compute_metrics(img1, rendered_img)\n",
    "    print(\"Neural BRDF Metrics:\", phong_metrics)\n",
    "else:\n",
    "    print(\"Error: Image shapes do not match.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last task describe in text your observations regarding:\n",
    "* How do the Phong and Neural BRDFs compare visually and in terms of metrics.\n",
    "* How do the two (or more) captured spheres compare. To work with the second material you can duplicate the fitting cells in this notebook or create another notebook for it.\n",
    "* Optionally, you can describe the effect of using different parametrizations, architectures, and loss functions for Neural BRDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the Phong and Neural BRDFs compare visually and in terms of metrics?\n",
    "\n",
    "The Phong BRDF image shows a bright, sharp highlight on one side of the sphere, which reflects where the light is coming from. The Phong model is creating a focused, shiny spot and produces a well-defined light effect. In comparison, the Neural BRDF image appears dimmer and more blurred, with softer highlights. The Neural BRDF has a more diffused light effect, making it harder to see the direction of the light source clearly.\n",
    "\n",
    "Looking at the metrics, the Phong BRDF has a much lower MSE score (21.85) compared to the Neural BRDF (701.05), showing that it is much closer to the reference image in terms of brightness and detail. Both images have similar SSIM scores, with the Phong BRDF being slightly higher (0.7573 vs. 0.7546). This means that while both models capture the general layout of light and dark areas similarly, the Phong BRDF does a better job of showing the sharpness and clarity of the light reflection. Overall, the Phong BRDF creates a more accurate and realistic image, while the Neural BRDF produces a softer, less defined appearance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
